
\begin{figure}
  \centering
  \includegraphics[width=0.40\paperwidth]{figures/lineRateModel.pdf}
  \caption{\label{fig:p4ModelTopo} Topology of the 10 Gb/s testbed 
  for real-time TCP benchmarks, using P4 to model FEC and faulty links.}
\end{figure}

\begin{figure*}[!ht]
\centering
\begin{minipage}[b]{0.31\linewidth}
\includegraphics[width=\linewidth]{figures/lossVsTput.pdf}
\caption{Iperf throughput.}
\label{fig:lossVsTput}
\end{minipage}
\hspace{.05in}
\begin{minipage}[b]{0.31\linewidth}
\includegraphics[width=\textwidth]{figures/lossVsWindow.pdf}
\caption{Iperf TCP window sizes.}
\label{fig:lossVsWindow}
\end{minipage}
\hspace{.05in}
\begin{minipage}[b]{0.31\linewidth}
  \centering
  \includegraphics[trim=6mm 6mm 0 0, width=0.95\textwidth]{figures/latency.pdf}
\caption{Latency at error rate=0. Boxes and whiskers show quartile and 1.5*quartile ranges.}
\label{fig:latency}
\end{minipage}
\end{figure*}


\section{Evaluation}
\label{sec:evaluation}
% We evaluate \OurSys using different types of traffic to measure its improvement
% to application-level behavior in the presence of lossy links.

We evaluated \OurSys with microbenchmarks and simulation.
% to answer these questions:
%\begin{itemize}

%\item What are the resource requirements for \OurSys with different levels of 
%error correction?

%\item How much does \OurSys improve application throughput across lossy links?

%\item What benefit can \OurSys have to networks at large?
%\end{itemize}



% We
% evaluate three possible deployment models for \OurSys: an FPGA external to
% the switch; a software implementation running on the switch CPU; and an ASIC  
% implementation integrated into the switch forwarding engine. 

\subsection{\OurSys Models}

\subsubsection{Line Rate P4 Model} 
\input{p4models}



\subsubsection{Event-based simulation}
We customized a fat-tree datacenter topology in ns-3~\cite{ns3-dcn} to
model (i)~a link with loss characteristics as described by Zhuo et
al.~\cite{Zhuo:2017:UMP:3098822.3098849}; and (ii)~FEC to support
transport protocols. In this model we experimented with end-to-end
error correction rather than link-layer, to simulate a more complex
implementation without incurring the burden of implementing it fully.

We simulated a 128-node fat-tree network with 10Gbps links where two
nodes communicate over TCP to transfer a 10MB file at 2Gbps. We found
that using FEC completely eliminated retransmissions (which consisted
  of 152, 23, and 2 packets for loss rates of $10^{-3}$, $10^{-4}$,
and $10^{-5}$ respectively). But achieving end-to-end reliability
over a lossy link with little sacrifice to latency came at a steep
end-to-end overhead of 20\%, since a parity packet was added for each
5 data packets. The approach described in this paper only adds
overhead on lossy links, rather than across paths that contain a
lossy link.


\subsection{Encoder Microbenchmarks}
\iffalse
Here we evaluate the implementation directly, not using a model.
Latency and throughput graphs for experiments involving different loss rates, and the encoder working on the CPU and FPGA.
Note: we have not optimized the CPU implementation.
\fi
We measured the performance of our current encoder implementation.  Packets
in a single flow with uniformly-distributed payload sizes of 64--1450 bytes are
supplied to the FPGA with the packet generator of DPDK 17.08.1. The encoder
processes the packet with parameters $k = 50$ and $h = 1$.
At the output, we measured a throughput of 9.3 Gbps over a 10-minute period,
nearly saturating the 10-Gbps link.

\iffalse
To ensure \OurSys's effect observed in the model are practical, we directly measured the 
full throughput of our encoder implementation in FPGA. For our benchmark, the encoder is configured to use
k=8 and h=4. Packets are generated by a tool based
on DPDK library, and are fed to the board (as is specified above~(\S\ref{sec:implementation})) through
a 10Gbps link. The average outgoing throughput measured during a 10 minutes test is 9.025Gbps.
Considering the overhead from other parts of the system, we believe the link is actually close
to being saturated, which is our basic assumption in evaluations.
\fi

For comparison, we also evaluated a reference CPU implementation, not 
optimized for performance. In the same deployment, its throughput 
measured 227Mbps on a single core, and 1399Mbps using all 8 physical cores of
our Xeon E5-2450L running at 1.8 GHz.

% As a contrast, we also evaluated a CPU implementation, which was not
% optimized for performance, and intended as a reference
% implementation. Under the same deployment, the throughput measured is



\subsection{FPGA resource consumption}
Table~\ref{tab:microbenchmarks} shows the resource requirements for the FPGA implementations of
\OurSys with different $k$ and $h$ parameters.  The resource requirements are
post-implementation utilization values reported by Xilinx Vivado.  We observe
that varying $k$ has a negligible effect on resource consumption, whereas BRAM
consumption has a strong dependence on $h$.  We believe that the BRAM consumption
can be further reduced because several arrays were overpartitioned.
%CPU cycles for the software
%implementation are measured using Linux performance counters and averaged over
%X packets,
%The timing statistics are measured using ingress and egress timestamps on
%the switch.

\begin{table}
% \footnotesize
\begin{center}
\small
% \resizebox{\linewidth}{!}{
\begin{tabular}{ l r r r r } 
\toprule
$(k, h)$ & $(25, 1)$ & $(25, 5)$ & $(25,10)$ & $(50, 1)$ \\
\midrule
%\emph{Software} & & & & \\
%\cmidrule{1-1}
%Cycles & & & & \\
%Proc. Time (ns) & & & & \\
%\midrule
%\emph{FPGA} & & & & \\
%\cmidrule{1-1}
BRAM (18Kb) & 135 (7\%) & 186 (10\%) & 248 (14\%) & 135 (7\%) \\
Flip-flop & 52420 (10\%) & 53415 (10\%) & 54497 (10\%) & 52420 (10\%) \\
LUT & 31372 (11\%) & 32439 (12\%) & 33136 (12\%) & 31368 (11\%) \\
%Proc. Time (ns) & \FIXME{?} & & & \\
\bottomrule
\end{tabular}
% }
\caption{Resource requirement %Comparison
of the FPGA %and CPU
implementation of \OurSys with different configurations.  Note that BRAMs are local memories,
and LUTs (lookup tables) are programmable gates.} % typically implement logic.}
\label{tab:microbenchmarks}
\end{center}
\end{table}

%We run \OurSys in 3 configurations: outside the switch, on the switch, and in the switch.
%Time how quickly \OurSys reacts to failing links.
