\section{Evaluation}
\label{sec:evaluation}
% We evaluate \OurSys using different types of traffic to measure its improvement
% to application-level behaviour in the presence of lossy links.

We evaluate \OurSys with microbenchmarks and network simulation to answer these questions:
\begin{itemize}

\item What are the resource requirements for \OurSys with different levels of 
error correction?

\item How much does \OurSys improve application throughput across lossy links?

%\item What benefit can \OurSys have to networks at large?
\end{itemize}

\subsection{Setup}
\begin{figure}
  \centering
  \includegraphics[width=0.3\paperwidth]{exp_topo.pdf}
  \caption{\label{fig:exp_topo} Testbed topology for benchmarks.}
\end{figure}

Figure~\ref{fig:exp_topo} depicts the testbed we used to benchmark \OurSys.
The two traffic generation hosts are each connected to different \OurSys
enabled switches, which are themselves connected by a 10 GbE link. The
switches are Wedge BF32-100X's with Barefoot Tofino~\cite{tofino} P4 
programmable forwarding engines. \lei{TODO: fix cite}

% We
% evaluate three possible deployment models for \OurSys: an FPGA external to
% the switch; a software implementation running on the switch CPU; and an ASIC  
% implementation integrated into the switch forwarding engine. 

\subsection{\OurSys models}

\subsubsection{Line Rate P4 Model.} 
\input{p4models}



\subsubsection{Event-based simulation}
We customised a fat-tree datacenter topology in ns3~\cite{ns3-dcn} to
model (i)~a link with loss characteristics as described by Zhuo et
al.~\cite{Zhuo:2017:UMP:3098822.3098849}; and (ii)~FEC to support
transport protocols. In this model we experimented with end-to-end
error correction rather than link-layer, to simulate a more complex
implementation without incurring the burden of implementing it fully.

Table~\ref{tab:ns3} shows our results for a simulation of a 128-node
fat-tree network with 10Gbps links where two nodes communicate over
TCP. We see that end-to-end FEC incurs significant end-to-end
overhead, making it less appealing than the link-layer design we opted
for in this paper.

\begin{table}
% \footnotesize
\begin{center}
\small
\begin{tabular}{c|cc|c}
\toprule
 & \multicolumn{2}{|c|}{Retransmitted packets} & \\
Link loss & no FEC & FEC & FEC Overhead (\%) \\
\midrule
$10^{-3}$ & 127 & 0 & 10 \\
$10^{-4}$ & 20  & 0 & 10 \\
$10^{-5}$ & 1   & 0 & 10 \\
\bottomrule
\end{tabular}
\caption{TCP retransmission behaviour with FEC, and FEC's overhead.
  We model sending 5MB at 2Gbps. For FEC we used a block size of 11,
  consisting of 10 data packets and one parity packet.\FIXME{Update
  this table with most recent numbers}
}
\label{tab:ns3}
\end{center}
\end{table}


\subsection{Encoder Microbenchmarks}
\iffalse
Here we evaluate the implementation directly, not using a model.
Latency and throughput graphs for experiments involving different loss rates, and the encoder working on the CPU and FPGA.
Note: we have not optimised the CPU implementation.
\fi
To ensure \OurSys's effect observed in the model are practical, we directly measured the 
full throughput of our encoder implementation in FPGA. For our benchmark, the encoder is configed to use
k=8 and h=4. Packets are generated by a tool based
on DPDK library, and are fed to the board (as is specified above~(\S\ref{sec:implementation})) through
a 10Gbps link. The average outgoing throughput measured during a 10 minutes test is 9.025Gbps.
Considering the overhead from other parts of the system, we believe the link is actually close
to being saturated, which is our basic assumption in evaluations.

As a contrast, we also evaluated the CPU implementaion (not optimised). Under the same deployment, the throughput measured is 
(\$CPUVALUE), (\$RATIO) lower compared to FPGA.
\lei{TODO: fill the number} \lei{\#Shrink Candidate\#}



\subsection{FPGA resource consumption}
Table~\ref{tab:microbenchmarks} shows the resource requirements for the FPGA implementations of
\OurSys with different H and K parameters.
%CPU cycles for the software
%implementation are measured using Linux performance counters and averaged over
%X packets,
FPGA requirements are obtained from the Xilinx compiler, and the
timing statistics are measured using ingress and egress timestamps on
the switch.


\begin{table}
% \footnotesize
\begin{center}
\small
% \resizebox{\linewidth}{!}{
\begin{tabular}{ l l l l l } 
\toprule
(H, K) & (5,3) & (10,3) & (20,3) & (40,3) \\
\midrule
\emph{Software} & & & & \\
\cmidrule{1-1}
Cycles & & & & \\
Processing Time (ns) & & & & \\
\midrule
\emph{FPGA} & & & & \\
\cmidrule{1-1}
??? & & & & \\
??? & & & & \\
Processing Time (ns) & & & & \\
\bottomrule

\end{tabular}
% }
\caption{Resource requirements for FPGA and CPU implementations of \OurSys with different configurations.}
\label{tab:microbenchmarks}
\end{center}
\end{table}


%We run \OurSys in 3 configurations: outside the switch, on the switch, and in the switch.
%Time how quickly \OurSys reacts to failing links.
