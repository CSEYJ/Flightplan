\section{Evaluation}
\label{sec:evaluation}
% We evaluate \OurSys using different types of traffic to measure its improvement
% to application-level behavior in the presence of lossy links.

We evaluated \OurSys with microbenchmarks and network simulation.
% to answer these questions:
%\begin{itemize}

%\item What are the resource requirements for \OurSys with different levels of 
%error correction?

%\item How much does \OurSys improve application throughput across lossy links?

%\item What benefit can \OurSys have to networks at large?
%\end{itemize}



% We
% evaluate three possible deployment models for \OurSys: an FPGA external to
% the switch; a software implementation running on the switch CPU; and an ASIC  
% implementation integrated into the switch forwarding engine. 

\subsection{\OurSys models}

\subsubsection{Line Rate P4 Model.} 
\input{p4models}



\subsubsection{Event-based simulation}
We customised a fat-tree datacenter topology in ns-3~\cite{ns3-dcn} to
model (i)~a link with loss characteristics as described by Zhuo et
al.~\cite{Zhuo:2017:UMP:3098822.3098849}; and (ii)~FEC to support
transport protocols. In this model we experimented with end-to-end
error correction rather than link-layer, to simulate a more complex
implementation without incurring the burden of implementing it fully.

We simulated a 128-node fat-tree network with 10Gbps links where two
nodes communicate over TCP to transfer a 10MB file at 2Gbps. We found
that using FEC completely eliminated retransmissions (which consisted
  of 152, 23, and 2 packets for loss rates of $10^{-3}$, $10^{-4}$,
and $10^{-5}$ respectively). But achieving end-to-end reliability
over a lossy link with little sacrifice to latency came at a steep
end-to-end overhead of 20\%, since a parity packet was added for each
5 data packets. The approach described in this paper only adds
overhead on lossy links, rather than across paths that contain a
lossy link.


\subsection{Encoder Microbenchmarks}
\iffalse
Here we evaluate the implementation directly, not using a model.
Latency and throughput graphs for experiments involving different loss rates, and the encoder working on the CPU and FPGA.
Note: we have not optimized the CPU implementation.
\fi
We measured the performance of our current encoder implementation, which has a
few restrictions.  The encoder parameters are fixed to $k = 8$ and $h = 4$,
and it handles only a single flow of packets with 1024-byte payloads.
Packets were supplied to the FPGA with the packet generator of DPDK 17.08.1.
At the output, we measured a throughput of 9.025 Gbps over a 10-minute period,
nearly saturating the 10-Gbps link.

\iffalse
To ensure \OurSys's effect observed in the model are practical, we directly measured the 
full throughput of our encoder implementation in FPGA. For our benchmark, the encoder is configured to use
k=8 and h=4. Packets are generated by a tool based
on DPDK library, and are fed to the board (as is specified above~(\S\ref{sec:implementation})) through
a 10Gbps link. The average outgoing throughput measured during a 10 minutes test is 9.025Gbps.
Considering the overhead from other parts of the system, we believe the link is actually close
to being saturated, which is our basic assumption in evaluations.
\fi

As a contrast, we also evaluated the CPU implementation (not optimized). Under the same deployment, the throughput measured is 
(\$CPUVALUE), (\$RATIO) lower compared to FPGA.
\lei{TODO: fill the number} \lei{\#Shrink Candidate\#}


\subsection{FPGA resource consumption}
Table~\ref{tab:microbenchmarks} shows the resource requirements for the FPGA implementations of
\OurSys with different $k$ and $h$ parameters.  The resource requirements are
post-implementation utilization values reported by Xilinx Vivado.  We observe
that varying $k$ has a negligible effect on resource consumption, whereas BRAM
consumption has a strong dependence on $h$.  We believe that the BRAM consumption
can be further reduced because several arrays were overpartitioned.
%CPU cycles for the software
%implementation are measured using Linux performance counters and averaged over
%X packets,
%The timing statistics are measured using ingress and egress timestamps on
%the switch.

\iffalse
\begin{table}
% \footnotesize
\begin{center}
\small
% \resizebox{\linewidth}{!}{
\begin{tabular}{ l l l l l l l } 
\toprule
$(k, h)$ & $(25, 1)$ & $(25, 5)$ & $(25,10)$ & $(50, 1)$ & $(50, 5)$ & $(50, 10)$ \\
\midrule
\emph{Software} & & & & & & \\
\cmidrule{1-1}
Cycles & & & & & & \\
Proc. Time (ns) & & & & & & \\
\midrule
\emph{FPGA} & & & & & & \\
\cmidrule{1-1}
BRAM (18Kb) & 135 (7\%) & 186 (10\%) & 248 (14\%) & 135 (7\%) & 186 (10\%) & 248 (14\%) \\
DSP & 0 (0\%) & 0 (0\%) & 0 (0\%) & 0 (0\%) & 0 (0\%) & 0 (0\%) \\
Flip-flop & 52420 (10\%) & 53415 (10\%) & 54497 (10\%) & 52420 (10\%) & 53416 (10\%) & 54496 (10\%) \\
LUT & 31372 (11\%) & 32439 (12\%) & 33136 (12\%) & 31368 (11\%) & 32479 (12\%) & 33215 (12\%) \\
Proc. Time (ns) & & & & & & \\
\bottomrule
\end{tabular}
% }
\caption{Resource requirements for FPGA and CPU implementations of \OurSys with different configurations.}
\label{tab:microbenchmarks}
\end{center}
\end{table}
\fi

\begin{table}
% \footnotesize
\begin{center}
\small
% \resizebox{\linewidth}{!}{
\begin{tabular}{ l r r r r } 
\toprule
$(k, h)$ & $(25, 1)$ & $(25, 5)$ & $(25,10)$ & $(50, 1)$ \\
\midrule
%\emph{Software} & & & & \\
%\cmidrule{1-1}
%Cycles & & & & \\
%Proc. Time (ns) & & & & \\
%\midrule
%\emph{FPGA} & & & & \\
%\cmidrule{1-1}
BRAM (18Kb) & 135 (7\%) & 186 (10\%) & 248 (14\%) & 135 (7\%) \\
DSP & 0 (0\%) & 0 (0\%) & 0 (0\%) & 0 (0\%) \\
Flip-flop & 52420 (10\%) & 53415 (10\%) & 54497 (10\%) & 52420 (10\%) \\
LUT & 31372 (11\%) & 32439 (12\%) & 33136 (12\%) & 31368 (11\%) \\
%Proc. Time (ns) & \FIXME{?} & & & \\
\bottomrule
\end{tabular}
% }
\caption{Resource and Performance measurement %Comparison
of the FPGA %and CPU
implementation of \OurSys with different configurations.}
\label{tab:microbenchmarks}
\end{center}
\end{table}

%We run \OurSys in 3 configurations: outside the switch, on the switch, and in the switch.
%Time how quickly \OurSys reacts to failing links.
