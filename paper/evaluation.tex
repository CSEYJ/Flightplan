\section{Evaluation}
\label{sec:evaluation}
% We evaluate \OurSys using different types of traffic to measure its improvement
% to application-level behaviour in the presence of lossy links.

We evaluate \OurSys with microbenchmarks and network simulation to answer these questions:
\begin{itemize}

\item What are the resource requirements for \OurSys with different levels of 
error correction?

\item How much does \OurSys improve application throughput across lossy links?

%\item What benefit can \OurSys have to networks at large?
\end{itemize}



% We
% evaluate three possible deployment models for \OurSys: an FPGA external to
% the switch; a software implementation running on the switch CPU; and an ASIC  
% implementation integrated into the switch forwarding engine. 

\subsection{\OurSys models}

\subsubsection{Line Rate P4 Model.} 
\input{p4models}



\subsubsection{Event-based simulation}
We customised a fat-tree datacenter topology in ns3~\cite{ns3-dcn} to
model (i)~a link with loss characteristics as described by Zhuo et
al.~\cite{Zhuo:2017:UMP:3098822.3098849}; and (ii)~FEC to support
transport protocols. In this model we experimented with end-to-end
error correction rather than link-layer, to simulate a more complex
implementation without incurring the burden of implementing it fully.

Table~\ref{tab:ns3} shows our results for a simulation of a 128-node
fat-tree network with 10Gbps links where two nodes communicate over
TCP. We see that end-to-end FEC incurs significant end-to-end
overhead, making it less appealing than the link-layer design we opted
for in this paper.

\begin{table}
% \footnotesize
\begin{center}
\small
\begin{tabular}{c|cc|c}
\toprule
 & \multicolumn{2}{|c|}{Retransmitted packets} & \\
Link loss & no FEC & FEC & FEC Overhead (\%) \\
\midrule
$10^{-3}$ & 152  & 0 & 20 \\
$10^{-4}$ & 23   & 0 & 20 \\
$10^{-5}$ & 2    & 0 & 20 \\
\bottomrule
\end{tabular}
\caption{TCP retransmission behaviour with FEC, and FEC's overhead.
  We model sending 10MB at 2Gbps. For FEC we used a block size of 6,
  consisting of 5 data packets and 1 parity packet.
}
\label{tab:ns3}
\end{center}
\end{table}


\subsection{Encoder Microbenchmarks}
\iffalse
Here we evaluate the implementation directly, not using a model.
Latency and throughput graphs for experiments involving different loss rates, and the encoder working on the CPU and FPGA.
Note: we have not optimised the CPU implementation.
\fi
We measured the performance of our current encoder implementation, which
has fixed parameters $k = 8$ and $h = 4$ and handles only packets with 1024-byte payloads.
Packets were supplied to the FPGA with the packet generator of DPDK 17.08.1.
At the output, we measured a throughput of 9.025 Gbps over a 10-minute period,
nearly saturating the 10-Gbps link.

\iffalse
To ensure \OurSys's effect observed in the model are practical, we directly measured the 
full throughput of our encoder implementation in FPGA. For our benchmark, the encoder is configed to use
k=8 and h=4. Packets are generated by a tool based
on DPDK library, and are fed to the board (as is specified above~(\S\ref{sec:implementation})) through
a 10Gbps link. The average outgoing throughput measured during a 10 minutes test is 9.025Gbps.
Considering the overhead from other parts of the system, we believe the link is actually close
to being saturated, which is our basic assumption in evaluations.
\fi

As a contrast, we also evaluated the CPU implementaion (not optimised). Under the same deployment, the throughput measured is 
(\$CPUVALUE), (\$RATIO) lower compared to FPGA.
\lei{TODO: fill the number} \lei{\#Shrink Candidate\#}


\subsection{FPGA resource consumption}
Table~\ref{tab:microbenchmarks} shows the resource requirements for the FPGA implementations of
\OurSys with different $k$ and $h$ parameters.  The device
has 1824 BRAMs, 548160 flip-flops, and 274080 LUTs.
%CPU cycles for the software
%implementation are measured using Linux performance counters and averaged over
%X packets,
FPGA requirements are obtained from the Xilinx synthesis tool, and the
timing statistics are measured using ingress and egress timestamps on
the switch.

\iffalse
\begin{table}
% \footnotesize
\begin{center}
\small
% \resizebox{\linewidth}{!}{
\begin{tabular}{ l l l l l l l } 
\toprule
$(k, h)$ & $(25, 1)$ & $(25, 5)$ & $(25,10)$ & $(50, 1)$ & $(50, 5)$ & $(50, 10)$ \\
\midrule
\emph{Software} & & & & & & \\
\cmidrule{1-1}
Cycles & & & & & & \\
Proc. Time (ns) & & & & & & \\
\midrule
\emph{FPGA} & & & & & & \\
\cmidrule{1-1}
BRAM (18Kb) & 135 (7\%) & 186 (10\%) & 248 (14\%) & 135 (7\%) & 186 (10\%) & 248 (14\%) \\
DSP & 0 (0\%) & 0 (0\%) & 0 (0\%) & 0 (0\%) & 0 (0\%) & 0 (0\%) \\
Flip-flop & 52420 (10\%) & 53415 (10\%) & 54497 (10\%) & 52420 (10\%) & 53416 (10\%) & 54496 (10\%) \\
LUT & 31372 (11\%) & 32439 (12\%) & 33136 (12\%) & 31368 (11\%) & 32479 (12\%) & 33215 (12\%) \\
Proc. Time (ns) & & & & & & \\
\bottomrule
\end{tabular}
% }
\caption{Resource requirements for FPGA and CPU implementations of \OurSys with different configurations.}
\label{tab:microbenchmarks}
\end{center}
\end{table}
\fi

\begin{table}
% \footnotesize
\begin{center}
\small
% \resizebox{\linewidth}{!}{
\begin{tabular}{ l r r r r } 
\toprule
$(k, h)$ & $(25, 1)$ & $(25, 5)$ & $(25,10)$ & $(50, 1)$ \\
\midrule
\emph{Software} & & & & \\
\cmidrule{1-1}
Cycles & & & & \\
Proc. Time (ns) & & & & \\
\midrule
\emph{FPGA} & & & & \\
\cmidrule{1-1}
BRAM (18Kb) & 135 (7\%) & 186 (10\%) & 248 (14\%) & 135 (7\%) \\
DSP & 0 (0\%) & 0 (0\%) & 0 (0\%) & 0 (0\%) \\
Flip-flop & 52420 (10\%) & 53415 (10\%) & 54497 (10\%) & 52420 (10\%) \\
LUT & 31372 (11\%) & 32439 (12\%) & 33136 (12\%) & 31368 (11\%) \\
Proc. Time (ns) & & & & \\
\bottomrule
\end{tabular}
% }
\caption{Resource requirements for FPGA and CPU implementations of \OurSys with different configurations.}
\label{tab:microbenchmarks}
\end{center}
\end{table}

%We run \OurSys in 3 configurations: outside the switch, on the switch, and in the switch.
%Time how quickly \OurSys reacts to failing links.
